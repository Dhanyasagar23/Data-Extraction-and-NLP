{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **DATA EXTRACTION AND NLP- BY DHANYA**"
      ],
      "metadata": {
        "id": "MAOrwByKhfU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tiUq7e3mhemH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "accessing my drive"
      ],
      "metadata": {
        "id": "4RshziUrhwCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "57naBzEWhVx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HDKbJbo8hZt2",
        "outputId": "ca6d7457-71f3-48a7-ee7e-f64d7938985e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat\n",
        "!pip install syllables"
      ],
      "metadata": {
        "id": "KSpE_Kgwhdre",
        "outputId": "fcee8615-dbf6-4e88-ee7b-0915827e0960",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.14.0 textstat-0.7.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting syllables\n",
            "  Downloading syllables-1.0.7-py3-none-any.whl (15 kB)\n",
            "Collecting cmudict<2.0.0,>=1.0.11\n",
            "  Downloading cmudict-1.0.13-py3-none-any.whl (939 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m939.3/939.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: importlib-resources<6.0.0,>=5.10.1 in /usr/local/lib/python3.10/dist-packages (from cmudict<2.0.0,>=1.0.11->syllables) (5.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->syllables) (3.15.0)\n",
            "Installing collected packages: importlib-metadata, cmudict, syllables\n",
            "Successfully installed cmudict-1.0.13 importlib-metadata-5.2.0 syllables-1.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import textstat\n",
        "from nltk.corpus import stopwords\n",
        "import syllables\n",
        "import string\n",
        "import re\n"
      ],
      "metadata": {
        "id": "PHstf6HhfsXe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "jzJOgOsjlduX",
        "outputId": "7633bc13-47f6-4018-c184-bdeccb6365cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load URLs from input file\n",
        "df = pd.read_excel('/content/drive/MyDrive/Input.xlsx')"
      ],
      "metadata": {
        "id": "q5eLUvddBVuQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Extraction part**"
      ],
      "metadata": {
        "id": "EB9aF3xxC_d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# loop through the URLs and extract the article text\n",
        "for index, row in df.iterrows():\n",
        "    url_id = str(row['URL_ID'])\n",
        "    url = row['URL']\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    text = \"\"\n",
        "    for para in soup.find_all('p'):\n",
        "        text += para.text + \"\\n\"\n",
        "    # save the text to a file with URL_ID as its name\n",
        "    with open(url_id + \".txt\", \"w\") as f:\n",
        "        f.write(text)\n"
      ],
      "metadata": {
        "id": "bkqjFl_i-qKc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**how texts look like**"
      ],
      "metadata": {
        "id": "Cqk95uLNh4DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"37.txt\"\n",
        "with open(filename, \"r\") as f:\n",
        "    contents = f.read()\n",
        "    print(contents)"
      ],
      "metadata": {
        "id": "YsTs0tGQCvmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/StopWords-20230503T100731Z-001.zip\""
      ],
      "metadata": {
        "id": "y1AFBTAGFaEW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing all stopwords**"
      ],
      "metadata": {
        "id": "x2i-2WfhiSuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load the stop words\n",
        "stop_words = []\n",
        "with open('/content/StopWords/StopWords_Names.txt',encoding='ISO-8859-1') as f:\n",
        "    stop_words += f.read().splitlines()\n",
        "with open('/content/StopWords/StopWords_Geographic.txt',encoding='ISO-8859-1') as f:\n",
        "    stop_words += f.read().splitlines()\n",
        "with open('/content/StopWords/StopWords_GenericLong.txt',encoding='ISO-8859-1') as f:\n",
        "    stop_words += f.read().splitlines()\n",
        "with open('/content/StopWords/StopWords_DatesandNumbers.txt',encoding='ISO-8859-1') as f:\n",
        "    stop_words += f.read().splitlines()\n",
        "with open('/content/StopWords/StopWords_Currencies.txt',encoding='ISO-8859-1') as f:\n",
        "    stop_words += f.read().splitlines()\n",
        "with open('/content/StopWords/StopWords_Auditor.txt',encoding='ISO-8859-1') as f:\n",
        "    stop_words += f.read().splitlines()\n",
        "with open('/content/StopWords/StopWords_Generic.txt',encoding='ISO-8859-1') as f:\n",
        "    stop_words += f.read().splitlines()\n",
        "\n",
        "# load the extracted text from the files and clean it\n",
        "for url_id in df['URL_ID']:\n",
        "    with open(f'{url_id}.txt', 'r') as f:\n",
        "        text = f.read().replace('\\n', '')\n",
        "        text_without_stopwords = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
        "    with open(f'{url_id}.txt', 'w') as f:\n",
        "        f.write(text_without_stopwords)\n"
      ],
      "metadata": {
        "id": "zucEsgCrEdY3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"37.txt\"\n",
        "with open(filename, \"r\") as f:\n",
        "    contents = f.read()\n",
        "    print(contents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W73diSqmI3Wu",
        "outputId": "0cfa6651-1f51-463b-a6cf-c0e3e052d36c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranking customer behaviours business strategyAlgorithmic trading multiple commodities markets, Forex, Metals, Energy, etc.Trading Bot FOREXPython model analysis sector-specific stock ETFs investment purposesPlaystore & Appstore Google Analytics (GA) Firebase Google Data Studio Mobile App KPI DashboardGoogle Local Service Ads LSA API Google BigQuery Google Data StudioAI Conversational Bot RASARecommendation System ArchitectureRise telemedicine Impact Livelihood 2040Rise e-health impact humans year 2030Rise e-health impact humans year 2030Rise telemedicine Impact Livelihood 2040AI/ML Predictive ModelingSolution Contact Centre ProblemsHow Setup Custom Domain Google App Engine Application?Code Review ChecklistIntroduction“If kills 10 million people decades, highly infectious virus war. missiles microbes.” Bill Gates’s remarks TED conference 2014, world avoided Ebola outbreak. new, unprecedented, invisible virus hit us, met overwhelmed unprepared healthcare system oblivious population. public health emergency demonstrated lack scientific consideration underlined alarming robust innovations health medical facilities. past years, artificial intelligence proven tangible potential healthcare sectors, clinical practices, translational medical biomedical research.After case detected China December 31st 2019, AI program developed BlueDot alerted world pandemic. quick realise AI’s ability analyse large chunks data detecting patterns identifying tracking carriers virus.Many tracing apps AI tabs people infected prevent risk cross-infection AI algorithms track patterns extract features classify categorise them.So AI that?IBM Watson, sophisticated AI works cloud computing natural language processing, prominently contributed healthcare sector global level. conversational AI, 2013, Watson helped recommending treatments patients suffering cancer ensure treatment optimum costs.Researchers Google Inc. showed AI system trained thousands images achieve physician-level sensitivity.By identifying molecular patterns disease status subtypes, gene expression, protein abundance levels, machine learning methods detect fatal diseases cancer early stage. Machine Learning (ML) techniques focus analyzing structured data, clustering patients’ traits infer probability disease outcomes. patient traits include masses data relating age, gender, disease history, disease-specific data diagnostic imaging gene expressions, etc, ML extract features data inputs constructing data analytical algorithms.ML algorithms supervised unsupervised. Unsupervised learning helps extracting features clustering similar features leads early detection diseases. Clustering principal component analysis enable grouping clustering similar traits maximize minimize similarity patients clusters. patient traits recorded multiple dimensions, genes, principal component analysis(PCA) creates apparatus reduce dimensions humans alone.Supervised learning considers outcomes subjects traits, correlates inputs outputs predict probability clinical event, expected disease level expected survival time, risk Down’s syndrome.Biomarker panels detect ovarian cancer, outperformed conventional statistical methods due machine learning. addition this, EHRs Bayesian networks, part supervised machine learning algorithms, predict clinical outcomes mortality respectively.Unstructured data clinical notes texts converted machine-readable structured data natural language processing(NLP). NLP works components: text processing classification. Text processing helps identifying series disease-relevant keywords clinical notes classification categorized normal abnormal cases. Chest screening ML NLP helped find abnormalities lungs provide treatment covid patients. Healthcare organizations NLP-based chatbots increase interactions patients, keeping mental health wellness check.Deep learning modern extension classical neural network techniques helps explore complex non-linear patterns data, algorithms convolution neural network, recurrent neural network, deep belief network, deep neural network enables accurate clinical prediction. genome interpretation, deep neural networks surpass conventional methods logistics regression support vector machines.Sepsis Watch AI system trained deep learning algorithms holds capability analyze 32 million data points create patient’s risk score identify early stages sepsis.Another method Learning-based Optimization Sampling Pattern( LOUPE) based integrating full resolution MRI scans convolutional neural network algorithm, helps creating accurate reconstructions.Robotic surgery widely considered delicate surgeries gynaecology prostate surgery. striking balance human decisions AI precision, robotic surgery reduces surgeon efficiency manually operated console. Thus, autonomous robotic surgery rise inventions robotic silicon fingers mimic sense touch surgeons identify organs, cut tissues, etc., robotic catheters navigate touching blood, tissue, valve.Researchers Children’s National Hospital, Washington developed AI called Smart Tissue Autonomous Robot (STAR), performs colon anastomosis ML-powered suturing tool, automatically detects patient’s breathing pattern apply suture correct point.Cloud computing healthcare helped retrieving sharing medical records safely reduction maintenance costs. technology doctors healthcare workers access detailed patient data helps speeding analysis ultimately leading care form accurate information, medications, therapies.How Biomedical research?Since AI analyze literature readability, concise biomedical research. ML algorithms NLP, AI accelerate screening indexing biomedical research, ranking literature interest researchers formulate test scientific hypotheses precisely quickly. Taking level, AI systems computational modelling assistant (CMA) helps researchers construct simulation models concepts mind. innovations majorly contributed topics tumour suppressor mechanisms protein-protein interaction information extraction.AI precision medicineSince precision medicine focuses healthcare interventions individuals groups patients based profile, AI devices pave practice efficiently. ML, complex algorithms large datasets predict create optimal treatment strategy.Deep learning neural networks process data healthcare apps close watch patient’s emotional state, food intake, health monitoring. “Omics” refers collective technologies exploring roles, relationships branches ending suffix “omics” genomics, proteomics, etc. Omics-based tests based machine learning algorithms find correlations predict treatment responses, ultimately creating personalized treatments individual patients. helps psychology neuro patientsFor psychologists studying creativity, AI promising classes experiments developing data structures programs exploring theories horizon. Studies show AI conduct therapy sessions, e-therapy sessions, assessments autonomously, assisting human practitioners before, during, sessions. Detection Computational Analysis Psychological Signal project ML, computer vision, NLP analyze language, physical gestures, social signals identify cues human distress. ground-breaking technology assesses soldiers returning combat recognizes require mental health support. future, combine data captured face-to-face interviews information sleeping, eating, online behaviours complete patient view.Stroke identificationStroke frequently occurring disease affects 500 million people worldwide. Thrombus, vessel cerebral infarction major (about 85%) stroke occurrence. recent years, AI techniques numerous stroke-related studies early detection timely treatment efficient outcome prediction solve problem. AI disposal, large amounts data rich information, complications real-life clinical questions addressed arena. Currently, ML algorithms- genetic fuzzy finite state machine PCA implemented build model building solution. include human activity recognition stage stroke onset detection stage. alert stroke message activated movement significantly normal pattern recorded. ML methods applied neuroimaging data assist disease evaluation predicting stroke treatment diagnosis.Patient MonitoringToday, market AI-based patient monitoring impressive monetarily enticing. evolving artificial sensors, smart technologies explores brain-computer interfaces nanorobotics. Companies smart-watches engaged people perform remote monitoring “patients”. obvious place start wearable embedded sensors, glucose monitors, pulse monitors, oximeters, ECG monitors. patient monitoring crucial, AI finds numerous applications chronic conditions, intensive care units, operating rooms, emergency rooms, cardiac wards timeless clinical decision-making measured seconds. advances started gain traction smart prosthetics implants. play impeccable role patient management post-surgery rehabilitation. Demographics, laboratory results vital signs predict cardiac arrest, transfer intensive care unit, death. addition, interpretable machine-learning model assist anesthesiologists predicting hypoxaemia events surgery. suggests deep-learning algorithms, raw patient-monitoring data avoid information overload alert overload enabling accurate clinical prediction timely decision-making. ConclusionConsidering vast range tasks AI do, evident holds deep potential improving patient outcomes skyrocketing levels. sophisticated algorithms AI bring revolution healthcare sector. facing challenges technology deliver promises, ethical measures, training physicians it, standard regulations etc, role AI transforming clinical practices ignored. biggest challenge integration AI daily practice. overcome period technologies mature making system enhanced effective.We provide intelligence, accelerate innovation implement technology extraordinary breadth depth global insights big data,data-driven dashboards, applications development, information management organizations combining unique, specialist services high-lvel human expertise.Contact us: hello@blackcoffer.com© Reserved, Blackcoffer(OPC) Pvt.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/drive/MyDrive/MasterDictionary-20230503T104416Z-001.zip\""
      ],
      "metadata": {
        "id": "4hTOM9huNJ-B"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a dictionary of Positive and Negative words and Extracting Derived variables**"
      ],
      "metadata": {
        "id": "m5fZpGdtiahH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load the positive and negative words\n",
        "pos_words = []\n",
        "with open('/content/MasterDictionary/positive-words.txt', 'r',encoding='ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        word = line.strip()\n",
        "        if word not in stop_words:\n",
        "            pos_words.append(word)\n",
        "\n",
        "neg_words = []\n",
        "with open('/content/MasterDictionary/negative-words.txt', 'r',encoding='ISO-8859-1') as f:\n",
        "    for line in f:\n",
        "        word = line.strip()\n",
        "        if word not in stop_words:\n",
        "            neg_words.append(word)\n",
        "\n",
        "##load the extracted text from the files and clean it\n",
        "text_list = []\n",
        "for url_id in df['URL_ID']:\n",
        "    with open(f'{url_id}.txt', 'r') as f:\n",
        "        text = f.read().replace('\\n', '')\n",
        "        text_list.append(' '.join([word for word in text.split() if word.lower() not in stop_words]))\n",
        "\n",
        "# tokenize the text and calculate scores\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "positive_scores = []\n",
        "negative_scores = []\n",
        "polarity_scores = []\n",
        "subjectivity_scores = []\n",
        "for text in text_list:\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_score = sum([1 if token in pos_words else 0 for token in tokens])\n",
        "    neg_score = sum([1 if token in neg_words else 0 for token in tokens]) * -1\n",
        "    polarity_score = (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
        "    subjectivity_score = (pos_score + abs(neg_score)) / (len(tokens) + 0.000001)\n",
        "    positive_scores.append(pos_score)\n",
        "    negative_scores.append(neg_score)\n",
        "    polarity_scores.append(polarity_score)\n",
        "    subjectivity_scores.append(subjectivity_score)\n",
        "\n",
        "# add the scores to the dataframe\n",
        "df['POSITIVE SCORE'] = positive_scores\n",
        "df['NEGATIVE SCORE'] = negative_scores\n",
        "df['POLARITY SCORE'] = polarity_scores\n",
        "df['SUBJECTIVITY SCORE'] = subjectivity_scores\n",
        "\n",
        "# print the dataframe\n",
        "\n"
      ],
      "metadata": {
        "id": "jqH2gyDdTWq8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'POSITIVE SCORE': 'Positive Score', 'NEGATIVE SCORE': 'Negative Score', 'POLARITY SCORE': 'Polarity Score', 'SUBJECTIVITY SCORE': 'Subjectivity Score'})\n"
      ],
      "metadata": {
        "id": "26Q1ugYLU3eL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of Readability**"
      ],
      "metadata": {
        "id": "gMvVmdlIirwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def calculate_readability(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(nltk.word_tokenize(text))\n",
        "    avg_sentence_len = num_words / num_sentences\n",
        "    num_complex_words = len([word for word in nltk.word_tokenize(text) if textstat.syllable_count(word) >= 3])\n",
        "    percent_complex_words = num_complex_words / num_words * 100\n",
        "    fog_index = 0.4 * (avg_sentence_len + percent_complex_words)\n",
        "    return avg_sentence_len, percent_complex_words, fog_index\n"
      ],
      "metadata": {
        "id": "kf2qCJPRjLdu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create empty lists to store the scores\n",
        "avg_sentence_lengths = []\n",
        "percent_complex_words = []\n",
        "fog_indices = []\n",
        "\n",
        "# loop over each text and calculate the readability scores\n",
        "for text in text_list:\n",
        "    avg_sentence_len, percent_complex, fog_index = calculate_readability(text)\n",
        "    avg_sentence_lengths.append(avg_sentence_len)\n",
        "    percent_complex_words.append(percent_complex)\n",
        "    fog_indices.append(fog_index)\n",
        "\n",
        "# add the new columns to the DataFrame\n",
        "df['AVG SENTENCE LENGTH'] = avg_sentence_lengths\n",
        "df['PERCENTAGE OF COMPLEX WORDS'] = percent_complex_words\n",
        "df['FOG INDEX'] = fog_indices\n"
      ],
      "metadata": {
        "id": "krNdeihqlAvh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average Number of Words Per Sentence**"
      ],
      "metadata": {
        "id": "rYUIoIT6jPDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# calculate average number of words per sentence\n",
        "total_words = 0\n",
        "total_sentences = 0\n",
        "for text in text_list:\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    total_words += sum(len(nltk.word_tokenize(sentence)) for sentence in sentences)\n",
        "    total_sentences += len(sentences)\n",
        "\n",
        "avg_words_per_sentence = total_words / total_sentences\n",
        "df['AVG NUMBER OF WORDS PER SENTENCE'] = total_words / total_sentences\n",
        "\n"
      ],
      "metadata": {
        "id": "bLwzpJBOwMZK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Complex Word Count**"
      ],
      "metadata": {
        "id": "AkhFFvLDjVYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('cmudict')\n",
        "d = cmudict.dict()\n",
        "\n",
        "\n",
        "\n",
        "def nsyl(word):\n",
        "    return [len(list(y for y in x if y[-1].isdigit())) for x in d.get(word.lower(), [[]])][0]\n",
        "\n",
        "def count_complex_words(text):\n",
        "    words = text.split()\n",
        "    complex_word_count = 0\n",
        "    \n",
        "    for word in words:\n",
        "        # remove punctuations from the word\n",
        "        word = word.strip('?,.!') \n",
        "        \n",
        "        # count the number of syllables in the word\n",
        "        syllable_count = nsyl(word)\n",
        "        \n",
        "        if syllable_count > 2:\n",
        "            complex_word_count += 1\n",
        "    \n",
        "    return complex_word_count\n"
      ],
      "metadata": {
        "id": "82z9JRVNZqra",
        "outputId": "2c50e3c7-ecb1-4efa-930f-3a8da93207f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "complex_word_counts = [count_complex_words(text) for text in text_list]\n",
        "df['COMPLEX WORD COUNT'] = complex_word_counts"
      ],
      "metadata": {
        "id": "KHOg6sTVZvRW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Count**"
      ],
      "metadata": {
        "id": "xmSbnHA-jd6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def count_words(text):\n",
        "    # remove punctuation from the text\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    \n",
        "    # tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    \n",
        "    # remove stop words from the list of words\n",
        "    words = [word.lower() for word in words if word.lower() not in stopwords.words('english')]\n",
        "    \n",
        "    # count the number of words in the list\n",
        "    word_count = len(words)\n",
        "    \n",
        "    return word_count\n"
      ],
      "metadata": {
        "id": "d5JzlNNsc4I-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_counts = [count_words(text) for text in text_list]\n",
        "df['WORD COUNT'] = cleaned_counts"
      ],
      "metadata": {
        "id": "eRsqgAUMa0E6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Syllable Count Per Word**"
      ],
      "metadata": {
        "id": "EY4Nfxs2jjyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_syllables(word):\n",
        "    # remove trailing \"e\" except for words with only one syllable\n",
        "    if len(word) > 2 and word[-1] == \"e\" and not word[-2] == \"l\":\n",
        "        word = word[:-1]\n",
        "\n",
        "    # count the number of vowels (excluding silent \"e\")\n",
        "    num_vowels = len(re.findall(r'[aeiouy]+', word, re.IGNORECASE))\n",
        "\n",
        "    # handle some exceptions\n",
        "    if word[-2:] == \"ed\" or word[-2:] == \"es\":\n",
        "        num_vowels -= 1\n",
        "    if word[-3:] == \"ied\" or word[-3:] == \"ies\":\n",
        "        num_vowels += 1\n",
        "\n",
        "    # every word has at least one syllable\n",
        "    return max(num_vowels, 1)\n"
      ],
      "metadata": {
        "id": "KerDO0iobD6W"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "syllables_counts = [count_syllables(text) for text in text_list]\n",
        "df['SYLLABLE PER WORD'] = syllables_counts"
      ],
      "metadata": {
        "id": "jKoFbpJEbhdS"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Personal Pronouns**"
      ],
      "metadata": {
        "id": "7MP7J3pEjoh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def count_personal_pronouns(text):\n",
        "    # define regex pattern to match personal pronouns\n",
        "    pattern = r\"\\b(I|we|We|My|my|Ours|Us|ours|us)\\b\"\n",
        "    \n",
        "    # find all matches of the pattern in the text\n",
        "    matches = re.findall(pattern, text)\n",
        "    \n",
        "    # return the count of matches\n",
        "    return len(matches)\n"
      ],
      "metadata": {
        "id": "JRopgiKSc6dD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pronouns_counts = [count_personal_pronouns(text) for text in text_list]\n",
        "df['PERSONAL PRONOUNS'] = pronouns_counts "
      ],
      "metadata": {
        "id": "reoMKyA1c-X3"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Average Word Length**"
      ],
      "metadata": {
        "id": "JpH10RSwjugJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_average_word_length(text):\n",
        "    # tokenize the text into individual words\n",
        "    words = text.split()\n",
        "    \n",
        "    # remove any punctuation marks from the words\n",
        "    words = [word.translate(str.maketrans('', '', string.punctuation)) for word in words]\n",
        "    \n",
        "    # calculate the total number of characters in all the words\n",
        "    total_characters = sum(len(word) for word in words)\n",
        "    \n",
        "    # calculate the total number of words\n",
        "    total_words = len(words)\n",
        "    \n",
        "    # divide the total number of characters by the total number of words to get the average word length\n",
        "    average_word_length = total_characters / total_words\n",
        "    \n",
        "    return average_word_length\n"
      ],
      "metadata": {
        "id": "2InDHWoHdWDo"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_counts = [calculate_average_word_length(text) for text in text_list]\n",
        "df['AVG WORD LENGTH'] = avg_counts"
      ],
      "metadata": {
        "id": "MMFKBiSEdaLJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "AE9EpLNOdiza",
        "outputId": "4e0369bd-6438-47c8-abf9-c0ec99a2df8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     URL_ID                                                URL  \\\n",
              "0        37  https://insights.blackcoffer.com/ai-in-healthc...   \n",
              "1        38  https://insights.blackcoffer.com/what-if-the-c...   \n",
              "2        39  https://insights.blackcoffer.com/what-jobs-wil...   \n",
              "3        40  https://insights.blackcoffer.com/will-machine-...   \n",
              "4        41  https://insights.blackcoffer.com/will-ai-repla...   \n",
              "..      ...                                                ...   \n",
              "109     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
              "110     147  https://insights.blackcoffer.com/the-future-of...   \n",
              "111     148  https://insights.blackcoffer.com/big-data-anal...   \n",
              "112     149  https://insights.blackcoffer.com/business-anal...   \n",
              "113     150  https://insights.blackcoffer.com/challenges-an...   \n",
              "\n",
              "     Positive Score  Negative Score  Polarity Score  Subjectivity Score  \\\n",
              "0                65             -31        2.823529            0.071217   \n",
              "1                58             -36        4.272727            0.093439   \n",
              "2                68             -34        3.000000            0.084788   \n",
              "3                58             -21        2.135135            0.078685   \n",
              "4                50             -22        2.571428            0.059950   \n",
              "..              ...             ...             ...                 ...   \n",
              "109              23             -27      -12.500003            0.074184   \n",
              "110              38             -11        1.814815            0.058824   \n",
              "111              29             -44       -4.866667            0.084884   \n",
              "112              29              -3        1.230769            0.068522   \n",
              "113              33             -38      -14.200003            0.095174   \n",
              "\n",
              "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
              "0              25.433962                    29.302671  21.894653   \n",
              "1              15.014925                    21.769384  14.713724   \n",
              "2              17.955224                    30.008313  19.185415   \n",
              "3              13.386667                    25.000000  15.354667   \n",
              "4              17.925373                    24.063281  16.795461   \n",
              "..                   ...                          ...        ...   \n",
              "109            18.216216                    28.189911  18.562451   \n",
              "110            21.358974                    25.570228  18.771681   \n",
              "111            14.098361                    27.209302  16.523065   \n",
              "112            29.187500                    32.334047  24.608619   \n",
              "113            12.032258                    26.541555  15.429525   \n",
              "\n",
              "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
              "0                           20.492587                 387        1135   \n",
              "1                           20.492587                 212         749   \n",
              "2                           20.492587                 351         981   \n",
              "3                           20.492587                 240         817   \n",
              "4                           20.492587                 304         954   \n",
              "..                                ...                 ...         ...   \n",
              "109                         20.492587                 160         569   \n",
              "110                         20.492587                 206         691   \n",
              "111                         20.492587                 239         712   \n",
              "112                         20.492587                 131         393   \n",
              "113                         20.492587                 198         621   \n",
              "\n",
              "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
              "0                 2989                  3         7.670807  \n",
              "1                 1819                  1         6.950327  \n",
              "2                 2667                  3         7.677157  \n",
              "3                 1997                  2         6.996346  \n",
              "4                 2337                  4         7.205882  \n",
              "..                 ...                ...              ...  \n",
              "109               1481                  2         7.734155  \n",
              "110               1744                  2         7.272727  \n",
              "111               1788                  2         7.201977  \n",
              "112               1131                  2         8.146465  \n",
              "113               1630                  2         7.287540  \n",
              "\n",
              "[114 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a4276ee4-1f4a-45f5-a52c-aeeaca86844f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL_ID</th>\n",
              "      <th>URL</th>\n",
              "      <th>Positive Score</th>\n",
              "      <th>Negative Score</th>\n",
              "      <th>Polarity Score</th>\n",
              "      <th>Subjectivity Score</th>\n",
              "      <th>AVG SENTENCE LENGTH</th>\n",
              "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
              "      <th>FOG INDEX</th>\n",
              "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
              "      <th>COMPLEX WORD COUNT</th>\n",
              "      <th>WORD COUNT</th>\n",
              "      <th>SYLLABLE PER WORD</th>\n",
              "      <th>PERSONAL PRONOUNS</th>\n",
              "      <th>AVG WORD LENGTH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>37</td>\n",
              "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
              "      <td>65</td>\n",
              "      <td>-31</td>\n",
              "      <td>2.823529</td>\n",
              "      <td>0.071217</td>\n",
              "      <td>25.433962</td>\n",
              "      <td>29.302671</td>\n",
              "      <td>21.894653</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>387</td>\n",
              "      <td>1135</td>\n",
              "      <td>2989</td>\n",
              "      <td>3</td>\n",
              "      <td>7.670807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38</td>\n",
              "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
              "      <td>58</td>\n",
              "      <td>-36</td>\n",
              "      <td>4.272727</td>\n",
              "      <td>0.093439</td>\n",
              "      <td>15.014925</td>\n",
              "      <td>21.769384</td>\n",
              "      <td>14.713724</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>212</td>\n",
              "      <td>749</td>\n",
              "      <td>1819</td>\n",
              "      <td>1</td>\n",
              "      <td>6.950327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39</td>\n",
              "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
              "      <td>68</td>\n",
              "      <td>-34</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.084788</td>\n",
              "      <td>17.955224</td>\n",
              "      <td>30.008313</td>\n",
              "      <td>19.185415</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>351</td>\n",
              "      <td>981</td>\n",
              "      <td>2667</td>\n",
              "      <td>3</td>\n",
              "      <td>7.677157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40</td>\n",
              "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
              "      <td>58</td>\n",
              "      <td>-21</td>\n",
              "      <td>2.135135</td>\n",
              "      <td>0.078685</td>\n",
              "      <td>13.386667</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>15.354667</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>240</td>\n",
              "      <td>817</td>\n",
              "      <td>1997</td>\n",
              "      <td>2</td>\n",
              "      <td>6.996346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>41</td>\n",
              "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
              "      <td>50</td>\n",
              "      <td>-22</td>\n",
              "      <td>2.571428</td>\n",
              "      <td>0.059950</td>\n",
              "      <td>17.925373</td>\n",
              "      <td>24.063281</td>\n",
              "      <td>16.795461</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>304</td>\n",
              "      <td>954</td>\n",
              "      <td>2337</td>\n",
              "      <td>4</td>\n",
              "      <td>7.205882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>146</td>\n",
              "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
              "      <td>23</td>\n",
              "      <td>-27</td>\n",
              "      <td>-12.500003</td>\n",
              "      <td>0.074184</td>\n",
              "      <td>18.216216</td>\n",
              "      <td>28.189911</td>\n",
              "      <td>18.562451</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>160</td>\n",
              "      <td>569</td>\n",
              "      <td>1481</td>\n",
              "      <td>2</td>\n",
              "      <td>7.734155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>147</td>\n",
              "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
              "      <td>38</td>\n",
              "      <td>-11</td>\n",
              "      <td>1.814815</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>21.358974</td>\n",
              "      <td>25.570228</td>\n",
              "      <td>18.771681</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>206</td>\n",
              "      <td>691</td>\n",
              "      <td>1744</td>\n",
              "      <td>2</td>\n",
              "      <td>7.272727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>148</td>\n",
              "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
              "      <td>29</td>\n",
              "      <td>-44</td>\n",
              "      <td>-4.866667</td>\n",
              "      <td>0.084884</td>\n",
              "      <td>14.098361</td>\n",
              "      <td>27.209302</td>\n",
              "      <td>16.523065</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>239</td>\n",
              "      <td>712</td>\n",
              "      <td>1788</td>\n",
              "      <td>2</td>\n",
              "      <td>7.201977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>149</td>\n",
              "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
              "      <td>29</td>\n",
              "      <td>-3</td>\n",
              "      <td>1.230769</td>\n",
              "      <td>0.068522</td>\n",
              "      <td>29.187500</td>\n",
              "      <td>32.334047</td>\n",
              "      <td>24.608619</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>131</td>\n",
              "      <td>393</td>\n",
              "      <td>1131</td>\n",
              "      <td>2</td>\n",
              "      <td>8.146465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>150</td>\n",
              "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
              "      <td>33</td>\n",
              "      <td>-38</td>\n",
              "      <td>-14.200003</td>\n",
              "      <td>0.095174</td>\n",
              "      <td>12.032258</td>\n",
              "      <td>26.541555</td>\n",
              "      <td>15.429525</td>\n",
              "      <td>20.492587</td>\n",
              "      <td>198</td>\n",
              "      <td>621</td>\n",
              "      <td>1630</td>\n",
              "      <td>2</td>\n",
              "      <td>7.287540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>114 rows × 15 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a4276ee4-1f4a-45f5-a52c-aeeaca86844f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a4276ee4-1f4a-45f5-a52c-aeeaca86844f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a4276ee4-1f4a-45f5-a52c-aeeaca86844f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# assume your dataframe is stored in a variable named df\n",
        "df.to_excel('my_data.xlsx', index=False)\n",
        "\n",
        "# download the xlsx file\n",
        "files.download('my_data.xlsx')"
      ],
      "metadata": {
        "id": "6oUDH0fykBuQ",
        "outputId": "7d2ccca6-ee10-47d3-bc03-beaca9763e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f8444d97-1623-4f22-8829-e537d98d2ea1\", \"my_data.xlsx\", 21709)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lqvMI1HIDD06"
      }
    }
  ]
}